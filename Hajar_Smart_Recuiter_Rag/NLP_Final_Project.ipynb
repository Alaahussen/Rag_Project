{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGhmnkPE3Bw7",
        "outputId": "6a600836-c3e4-4ace-ecc6-bb9fb5d5a842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.32.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG5qH3s63BwB",
        "outputId": "ac12cfd7-b3f4-4779-910c-93fe31dd5148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ],
      "source": [
        "pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFiT9a0Q3Brt",
        "outputId": "d69f3434-189a-40e9-edd5-6ade7a946eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.63)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.44)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.11.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y0QLI-y3Bqx",
        "outputId": "2bbfa7fb-689f-4c3f-809c-db17846e677f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.1.5)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.62 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.63)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.5)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.3.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (4.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.72.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain-google-genai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixY5gcet3Bmn",
        "outputId": "d0852f97-011b-4464-bc18-56f5a277c8ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### importing library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "R6i3WRWO3Bld"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA, LLMChain\n",
        "from langchain.schema import Document\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "SAexsYIk15NT",
        "outputId": "8967ddb1-8b94-4b95-bf8a-71c52985da12"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6ce8f4bf-7924-45f7-b43d-2e534996e218\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6ce8f4bf-7924-45f7-b43d-2e534996e218\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Resumes.rar to Resumes (1).rar\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbjo9UL928gG",
        "outputId": "6e2a4cea-8339-438c-fd84-a32efaf93f0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unrar is already the newest version (1:6.1.5-1ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install unrar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdk5J8UK28fQ",
        "outputId": "6481a6a0-a7b7-4339-be04-84752320d48c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "UNRAR 6.11 beta 1 freeware      Copyright (c) 1993-2022 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/Resumes.rar\n",
            "\n",
            "\n",
            "Would you like to replace the existing file /content/Resumes/Resumes/Abdallah_Mahmoud_CV.pdf\n",
            "186842 bytes, modified on 2025-05-07 22:09\n",
            "with a new one\n",
            "186842 bytes, modified on 2025-05-07 22:09\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit \n",
            "User break\n",
            "\n",
            "User break\n"
          ]
        }
      ],
      "source": [
        "!unrar x /content/Resumes.rar /content/Resumes/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7CW785Z28az",
        "outputId": "bb4a3bd5-4087-4b4b-f681-070b4b737f3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Resumes/Resumes/Hajar_ITI_Resume (8).pdf\n",
            "/content/Resumes/Resumes/Aya Attia_(cv).pdf\n",
            "/content/Resumes/Resumes/Adham Assy - Resume .pdf\n",
            "/content/Resumes/Resumes/Resume - Ali Ayman.pdf\n",
            "/content/Resumes/Resumes/Mohamed_Salama_AI_engineer.pdf\n",
            "/content/Resumes/Resumes/Mohamed Kamal Elsharkawy.pdf\n",
            "/content/Resumes/Resumes/Abdallah_Mahmoud_CV.pdf\n",
            "/content/Resumes/Resumes/Salma-Ibrahim-Zaher-Ai.pdf\n",
            "/content/Resumes/Resumes/Ali_Salama_resume.pdf\n",
            "/content/Resumes/Resumes/Mennatullah-Tarek-CV.pdf\n",
            "/content/Resumes/Resumes/IsraaAbdelghany_AI&ML_Engineer.pdf\n",
            "/content/Resumes/Resumes/Alaa_Hussien_cv.pdf\n",
            "/content/Resumes/Resumes/Nagwa Mohamed.pdf\n",
            "/content/Resumes/Resumes/amr_CV_1.2.pdf\n",
            "/content/Resumes/Resumes/Alaa_s CV.pdf\n",
            "/content/Resumes/Resumes/Reham_CV_April_2025-1-.pdf\n",
            "/content/Resumes/Resumes/Yasmin_Kadry_CV.pdf\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "resume_dir = \"/content/Resumes\"\n",
        "for root, dirs, files in os.walk(resume_dir):\n",
        "    for file in files:\n",
        "        print(os.path.join(root, file))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Nj_6T8Tu28Z-"
      },
      "outputs": [],
      "source": [
        "# Set your API key\n",
        "api_key = \"AIzaSyBU83tqvEatbCbq3rFMGChtkcumVei-u-4\"  # Replace with your actual Google API key\n",
        "genai.configure(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "ZyX0D6Mc28Vq"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/Resumes/Resumes\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "E4Lrt94T28U1"
      },
      "outputs": [],
      "source": [
        "# 1. Document Loading Function\n",
        "def load_documents(directory_path):\n",
        "    \"\"\"Load all PDF documents from a directory\"\"\"\n",
        "    documents = []\n",
        "    candidate_names = []\n",
        "\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            path = os.path.join(directory_path, filename)\n",
        "            loader = PyMuPDFLoader(path)\n",
        "\n",
        "            # Extract documents\n",
        "            docs = loader.load()\n",
        "\n",
        "            # Extract candidate name from filename\n",
        "            candidate_name = os.path.splitext(filename)[0]\n",
        "            candidate_names.append(candidate_name)\n",
        "\n",
        "            # Add metadata\n",
        "            for doc in docs:\n",
        "                doc.metadata[\"candidate_name\"] = candidate_name\n",
        "                doc.metadata[\"source_file\"] = filename\n",
        "\n",
        "            documents.extend(docs)\n",
        "            print(f\"Loaded resume: {filename}\")\n",
        "\n",
        "    print(f\"Total documents loaded: {len(documents)}\")\n",
        "    return documents, candidate_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "uZ5GV6tG28Q4"
      },
      "outputs": [],
      "source": [
        "# load_documents(data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### chuncking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "uceYGQck5Wrz"
      },
      "outputs": [],
      "source": [
        "# 2. Document Splitting Function\n",
        "def split_by_chunk_size(documents, chunk_size=1000):\n",
        "    \"\"\"Split each unique resume into chunks based on a fixed character size.\"\"\"\n",
        "    all_chunks = []\n",
        "\n",
        "    # Group documents by candidate\n",
        "    candidate_docs = {}\n",
        "    for doc in documents:\n",
        "        candidate_name = doc.metadata.get(\"candidate_name\", \"\")\n",
        "        if candidate_name not in candidate_docs:\n",
        "            candidate_docs[candidate_name] = []\n",
        "        candidate_docs[candidate_name].append(doc)\n",
        "\n",
        "    # Process each candidate's documents\n",
        "    for candidate_name, docs in candidate_docs.items():\n",
        "        # Combine all pages into one text\n",
        "        full_text = \" \".join([doc.page_content for doc in docs])\n",
        "        source_file = docs[0].metadata.get(\"source_file\", \"\")\n",
        "\n",
        "        resume_chunks = []\n",
        "        num_chunks = (len(full_text) + chunk_size - 1) // chunk_size  # total chunk count\n",
        "\n",
        "        for i in range(0, len(full_text), chunk_size):\n",
        "            chunk_text = full_text[i:i + chunk_size]\n",
        "            chunk_index = i // chunk_size\n",
        "\n",
        "            chunk = Document(\n",
        "                page_content=chunk_text,\n",
        "                metadata={\n",
        "                    \"candidate_name\": candidate_name,\n",
        "                    \"source_file\": source_file,\n",
        "                    \"chunk_index\": chunk_index,\n",
        "                    \"total_chunks\": num_chunks\n",
        "                }\n",
        "            )\n",
        "            resume_chunks.append(chunk)\n",
        "\n",
        "        all_chunks.extend(resume_chunks)\n",
        "        print(f\"âœ… Split resume for {candidate_name} into {len(resume_chunks)} chunks\")\n",
        "\n",
        "    print(f\"\\nðŸ“„ Total chunks created: {len(all_chunks)}\")\n",
        "    return all_chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2BD96Rc5Wm_",
        "outputId": "14ef6033-402e-4502-e549-6c7ff7028cc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading documents...\n",
            "Loaded resume: Hajar_ITI_Resume (8).pdf\n",
            "Loaded resume: Aya Attia_(cv).pdf\n",
            "Loaded resume: Adham Assy - Resume .pdf\n",
            "Loaded resume: Resume - Ali Ayman.pdf\n",
            "Loaded resume: Mohamed_Salama_AI_engineer.pdf\n",
            "Loaded resume: Mohamed Kamal Elsharkawy.pdf\n",
            "Loaded resume: Abdallah_Mahmoud_CV.pdf\n",
            "Loaded resume: Salma-Ibrahim-Zaher-Ai.pdf\n",
            "Loaded resume: Ali_Salama_resume.pdf\n",
            "Loaded resume: Mennatullah-Tarek-CV.pdf\n",
            "Loaded resume: IsraaAbdelghany_AI&ML_Engineer.pdf\n",
            "Loaded resume: Alaa_Hussien_cv.pdf\n",
            "Loaded resume: Nagwa Mohamed.pdf\n",
            "Loaded resume: amr_CV_1.2.pdf\n",
            "Loaded resume: Alaa_s CV.pdf\n",
            "Loaded resume: Reham_CV_April_2025-1-.pdf\n",
            "Loaded resume: Yasmin_Kadry_CV.pdf\n",
            "Total documents loaded: 37\n",
            "Candidates: Hajar_ITI_Resume (8), Aya Attia_(cv), Adham Assy - Resume , Resume - Ali Ayman, Mohamed_Salama_AI_engineer, Mohamed Kamal Elsharkawy, Abdallah_Mahmoud_CV, Salma-Ibrahim-Zaher-Ai, Ali_Salama_resume, Mennatullah-Tarek-CV, IsraaAbdelghany_AI&ML_Engineer, Alaa_Hussien_cv, Nagwa Mohamed, amr_CV_1.2, Alaa_s CV, Reham_CV_April_2025-1-, Yasmin_Kadry_CV\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3. Load and Process Documents\n",
        "print(\"Loading documents...\")\n",
        "documents, candidate_names = load_documents(data_dir)\n",
        "print(f\"Candidates: {', '.join(candidate_names)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYL7YER35WmC",
        "outputId": "ea3c0752-94d4-40fe-93d1-66898cc7c730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Splitting documents into chunks...\n",
            "âœ… Split resume for Hajar_ITI_Resume (8) into 7 chunks\n",
            "âœ… Split resume for Aya Attia_(cv) into 5 chunks\n",
            "âœ… Split resume for Adham Assy - Resume  into 5 chunks\n",
            "âœ… Split resume for Resume - Ali Ayman into 6 chunks\n",
            "âœ… Split resume for Mohamed_Salama_AI_engineer into 5 chunks\n",
            "âœ… Split resume for Mohamed Kamal Elsharkawy into 6 chunks\n",
            "âœ… Split resume for Abdallah_Mahmoud_CV into 4 chunks\n",
            "âœ… Split resume for Salma-Ibrahim-Zaher-Ai into 3 chunks\n",
            "âœ… Split resume for Ali_Salama_resume into 6 chunks\n",
            "âœ… Split resume for Mennatullah-Tarek-CV into 4 chunks\n",
            "âœ… Split resume for IsraaAbdelghany_AI&ML_Engineer into 7 chunks\n",
            "âœ… Split resume for Alaa_Hussien_cv into 5 chunks\n",
            "âœ… Split resume for Nagwa Mohamed into 3 chunks\n",
            "âœ… Split resume for amr_CV_1.2 into 4 chunks\n",
            "âœ… Split resume for Alaa_s CV into 4 chunks\n",
            "âœ… Split resume for Reham_CV_April_2025-1- into 3 chunks\n",
            "âœ… Split resume for Yasmin_Kadry_CV into 2 chunks\n",
            "\n",
            "ðŸ“„ Total chunks created: 79\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 4. Split Documents into Chunks\n",
        "print(\"\\nSplitting documents into chunks...\")\n",
        "split_docs = split_by_chunk_size(documents, chunk_size=1000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLedtkLq5Whf",
        "outputId": "68146fef-f81e-49b2-8f1c-bcfec77b01c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample chunks:\n",
            "Chunk 1 - Candidate: Hajar_ITI_Resume (8), Section: 1/7\n",
            "Hajar Elsayed Elbehairy \n",
            "hagarelbehairy3@gmail.com| 01003554652 /kafr Elshiekh(easy to relocated) \n",
            "linkedin.com/in/hajar-elbehairy-| github.com/HajarE...\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 2 - Candidate: Hajar_ITI_Resume (8), Section: 2/7\n",
            "ls like MLflow and Hugging Face, including developing GANs for content generation and NLP \n",
            "models with attention mechanisms. \n",
            "ï‚· \n",
            "Designed and optimize...\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 3 - Candidate: Hajar_ITI_Resume (8), Section: 3/7\n",
            "resented at ITAF 2024 International Conference \"Foresights of Technology and Management within Emerging Developments\" \n",
            "Technical Skills \n",
            " \n",
            "ï‚· \n",
            "Programm...\n",
            "----------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 5. Display Some Chunks for Verification\n",
        "print(\"\\nSample chunks:\")\n",
        "for i, chunk in enumerate(split_docs[:3]):  # Just show the first 3 chunks\n",
        "    candidate = chunk.metadata.get(\"candidate_name\")\n",
        "    chunk_index = chunk.metadata.get(\"chunk_index\")\n",
        "    total_chunks = chunk.metadata.get(\"total_chunks\")\n",
        "    print(f\"Chunk {i+1} - Candidate: {candidate}, Section: {chunk_index+1}/{total_chunks}\")\n",
        "    print(f\"{chunk.page_content[:150]}...\\n{'-'*40}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vectore database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0pQHqh-5Wgq",
        "outputId": "572f81fb-c79c-4ec4-d2e2-8764d294eb14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating vector store...\n",
            "Vector store saved locally\n"
          ]
        }
      ],
      "source": [
        "# 6. Setup Embeddings and Vector Store\n",
        "print(\"Creating vector store...\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(split_docs, embedding=embeddings)\n",
        "vectorstore.save_local(\"faiss_resume_vectorstore\")\n",
        "print(\"Vector store saved locally\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "nUN7p7w828Pu"
      },
      "outputs": [],
      "source": [
        "# 7. Direct API Access for LLM (bypassing LangChain)\n",
        "def generate_response(prompt):\n",
        "    \"\"\"Generate response using Google Generative AI directly\"\"\"\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "QHNzXJy-7YMy"
      },
      "outputs": [],
      "source": [
        "# 8. Query Functions\n",
        "def query_all_resumes(question, k=3):\n",
        "    \"\"\"Query information across all resumes\"\"\"\n",
        "    # Get relevant documents\n",
        "    docs = vectorstore.similarity_search(question, k=k)\n",
        "\n",
        "    # Extract text content from documents\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Create the prompt\n",
        "    prompt = f\"\"\"\n",
        "You are an expert HR assistant helping with candidate screening.\n",
        "\n",
        "Given the resume information below, answer the HR professional's question accurately and professionally.\n",
        "\n",
        "---\n",
        "ðŸ“„ Resume Information:\n",
        "{context}\n",
        "---\n",
        "\n",
        "â“ HR Question:\n",
        "{question}\n",
        "\n",
        "---\n",
        "ðŸŽ¯ Your Task:\n",
        "- Provide a clear and direct answer based only on the resume.\n",
        "- If the answer is not explicitly mentioned, respond with: \"This information is not available in the resume.\"\n",
        "- Use formal HR language.\n",
        "- Be concise but thorough.\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "    # Get response directly from Google API\n",
        "    result = generate_response(prompt)\n",
        "\n",
        "    return {\n",
        "        \"result\": result,\n",
        "        \"source_documents\": docs\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "v5nfNCpr7YLz"
      },
      "outputs": [],
      "source": [
        "def query_specific_candidate(candidate_name, question, k=3):\n",
        "    \"\"\"Query information about a specific candidate\"\"\"\n",
        "    # Get relevant documents for this candidate\n",
        "    # We need to search more docs initially to find ones from the right candidate\n",
        "    all_docs = vectorstore.similarity_search(question, k=k*3)\n",
        "\n",
        "    # Filter for only the candidate we want\n",
        "    candidate_docs = [doc for doc in all_docs if doc.metadata.get(\"candidate_name\") == candidate_name]\n",
        "\n",
        "    # If we don't have enough docs, get more\n",
        "    if len(candidate_docs) < k and len(candidate_docs) > 0:\n",
        "        candidate_docs = candidate_docs[:k]\n",
        "\n",
        "    # Extract text content from documents\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in candidate_docs])\n",
        "\n",
        "    # Create the prompt\n",
        "    prompt = f\"\"\"\n",
        "You are an expert HR assistant helping with candidate screening.\n",
        "\n",
        "Given the resume information below for candidate {candidate_name}, answer the HR professional's question accurately and professionally.\n",
        "\n",
        "---\n",
        "ðŸ“„ Resume Information:\n",
        "{context}\n",
        "---\n",
        "\n",
        "â“ HR Question:\n",
        "{question}\n",
        "\n",
        "---\n",
        "ðŸŽ¯ Your Task:\n",
        "- Provide a clear and direct answer based only on the resume.\n",
        "- If the answer is not explicitly mentioned, respond with: \"This information is not available in the resume.\"\n",
        "- Use formal HR language.\n",
        "- Be concise but thorough.\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "    # Get response directly from Google API\n",
        "    result = generate_response(prompt)\n",
        "\n",
        "    return {\n",
        "        \"result\": result,\n",
        "        \"source_documents\": candidate_docs\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### extend Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "fQoBnWyv7YGx"
      },
      "outputs": [],
      "source": [
        "def rank_candidates(criteria, top_n=5, k_per_candidate=3):\n",
        "    \"\"\"Evaluate and rank candidates based on specific criteria\"\"\"\n",
        "    # Evaluate each candidate\n",
        "    evaluations = []\n",
        "\n",
        "    for candidate in candidate_names:\n",
        "        print(f\"Evaluating: {candidate}\")\n",
        "\n",
        "        # Get all docs for this candidate that match the criteria\n",
        "        all_docs = vectorstore.similarity_search(criteria, k=k_per_candidate*3)\n",
        "        candidate_docs = [doc for doc in all_docs if doc.metadata.get(\"candidate_name\") == candidate]\n",
        "        candidate_docs = candidate_docs[:k_per_candidate]  # Take the top k\n",
        "\n",
        "        # Extract text content from documents\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in candidate_docs])\n",
        "\n",
        "        # Create evaluation prompt\n",
        "        prompt = f\"\"\"\n",
        "You are an expert HR assistant evaluating a candidate for a position.\n",
        "\n",
        "Given the resume information below for candidate {candidate}, evaluate the candidate specifically on this criteria:\n",
        "{criteria}\n",
        "\n",
        "---\n",
        "ðŸ“„ Resume Information:\n",
        "{context}\n",
        "---\n",
        "\n",
        "Provide a score from 1-10 and a detailed justification.\n",
        "Your response must follow this format:\n",
        "CANDIDATE: {candidate}\n",
        "SCORE: [1-10]\n",
        "JUSTIFICATION: [Detailed explanation]\n",
        "\"\"\"\n",
        "\n",
        "        # Get response directly from Google API\n",
        "        evaluation = generate_response(prompt)\n",
        "        evaluations.append(evaluation)\n",
        "\n",
        "    # Combine all evaluations\n",
        "    all_evaluations = \"\\n\\n\".join(evaluations)\n",
        "\n",
        "    # Create ranking prompt\n",
        "    ranking_prompt = f\"\"\"\n",
        "You are an expert HR assistant ranking candidates for a position.\n",
        "\n",
        "You need to rank the top {top_n} candidates based on this criteria:\n",
        "{criteria}\n",
        "\n",
        "Here are the evaluations of each candidate:\n",
        "{all_evaluations}\n",
        "\n",
        "List the top {top_n} candidates in order, with their scores and a brief summary of why they are ranked in that position.\n",
        "Format your response as a ranked list.\n",
        "\"\"\"\n",
        "\n",
        "    # Get final ranking directly from Google API\n",
        "    ranking = generate_response(ranking_prompt)\n",
        "\n",
        "    return ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "uIXeHVN37YFz",
        "outputId": "0ea6d992-f92c-4c2e-91b2-7496a486650c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "QUERY ACROSS ALL RESUMES\n",
            "==================================================\n",
            "Question: What programming languages do the candidates know?\n",
            "Answer: The candidate is proficient in Python, C, C++, SQL, Java, and NoSQL.\n",
            "\n",
            "\n",
            "Sources:\n",
            "Source 1 (Mohamed_Salama_AI_engineer, Section 2/5):\n",
            " \n",
            "Intelligence and Machine learning track, Mansoura Branch. \n",
            "      (Oct 2024 â€“ June 2025)  \n",
            " \n",
            "ï‚· Bachelor's in Artificial Intelligence, Kafr-El-Sheikh ...\n",
            "\n",
            "Source 2 (Resume - Ali Ayman, Section 5/6):\n",
            "os to extract meaningful information through techniques like image segmentation, object\n",
            "detection, OCR, facial recognition, pose estimation, object tr...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run this section to test the system\n",
        "\n",
        "# Example 1: Query all resumes\n",
        "question = \"What programming languages do the candidates know?\"\n",
        "result = query_all_resumes(question)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"QUERY ACROSS ALL RESUMES\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {result['result']}\")\n",
        "\n",
        "print(\"\\nSources:\")\n",
        "for i, doc in enumerate(result[\"source_documents\"][:2], 1):\n",
        "    candidate = doc.metadata.get(\"candidate_name\", \"Unknown\")\n",
        "    chunk_index = doc.metadata.get(\"chunk_index\", \"Unknown\")\n",
        "    total_chunks = doc.metadata.get(\"total_chunks\", \"Unknown\")\n",
        "    print(f\"Source {i} ({candidate}, Section {chunk_index+1}/{total_chunks}):\")\n",
        "    print(f\"{doc.page_content[:150]}...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "QgtlPQfS7YBf",
        "outputId": "2a68237c-4654-403a-d8d6-f4bd54aafd2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "QUERY FOR CANDIDATE: Hajar_ITI_Resume (8)\n",
            "==================================================\n",
            "Question: What is this candidate's work experience?\n",
            "Answer: This information is not available in the resume.\n",
            "\n",
            "\n",
            "Sources:\n"
          ]
        }
      ],
      "source": [
        "# Example 2: Query specific candidate (using the first candidate as an example)\n",
        "if candidate_names:\n",
        "    specific_candidate = candidate_names[0]\n",
        "    specific_question = \"What is this candidate's work experience?\"\n",
        "    specific_result = query_specific_candidate(specific_candidate, specific_question)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"QUERY FOR CANDIDATE: {specific_candidate}\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Question: {specific_question}\")\n",
        "    print(f\"Answer: {specific_result['result']}\")\n",
        "\n",
        "    print(\"\\nSources:\")\n",
        "    for i, doc in enumerate(specific_result[\"source_documents\"][:2], 1):\n",
        "        chunk_index = doc.metadata.get(\"chunk_index\", \"Unknown\")\n",
        "        total_chunks = doc.metadata.get(\"total_chunks\", \"Unknown\")\n",
        "        print(f\"Source {i} (Section {chunk_index+1}/{total_chunks}):\")\n",
        "        print(f\"{doc.page_content[:150]}...\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "i9u_t8Pm7YAi",
        "outputId": "d271b783-9dcc-40c4-ade4-91c508cd9d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "RANKING CANDIDATES\n",
            "==================================================\n",
            "Criteria: computer vision ,object detection, and image segmentation\n",
            "Processing...\n",
            "Evaluating: Hajar_ITI_Resume (8)\n",
            "Evaluating: Aya Attia_(cv)\n",
            "Evaluating: Adham Assy - Resume \n",
            "Evaluating: Resume - Ali Ayman\n",
            "Evaluating: Mohamed_Salama_AI_engineer\n",
            "Evaluating: Mohamed Kamal Elsharkawy\n",
            "Evaluating: Abdallah_Mahmoud_CV\n",
            "Evaluating: Salma-Ibrahim-Zaher-Ai\n",
            "Evaluating: Ali_Salama_resume\n",
            "Evaluating: Mennatullah-Tarek-CV\n",
            "Evaluating: IsraaAbdelghany_AI&ML_Engineer\n",
            "Evaluating: Alaa_Hussien_cv\n",
            "Evaluating: Nagwa Mohamed\n",
            "Evaluating: amr_CV_1.2\n",
            "Evaluating: Alaa_s CV\n",
            "Evaluating: Reham_CV_April_2025-1-\n",
            "Evaluating: Yasmin_Kadry_CV\n",
            "\n",
            "Results:\n",
            "Top 5 Candidates Ranked by Computer Vision, Object Detection, and Image Segmentation Proficiency:\n",
            "\n",
            "\n",
            "1. **amr_CV_1.2 (Score: 8):**  amr demonstrates strong practical skills across all three criteria through diverse projects (lung CT scan segmentation, self-driving robot with object detection and hand gesture recognition, sign language translator).  The projects showcase a good understanding of various algorithms and libraries. While lacking explicit quantitative results, the breadth and complexity of the work justify the high ranking.\n",
            "\n",
            "\n",
            "2. **Mohamed Kamal Elsharkawy (Score: 7):** Mohamed showcases impressive practical object detection skills with his \"Underwater Plastic Detector\" project, which involves real-time processing and deployment as a REST API.  The use of YOLOv8s demonstrates familiarity with advanced techniques.  However, a lack of detailed information on image segmentation limits his score.\n",
            "\n",
            "\n",
            "3. **IsraaAbdelghany_AI&ML_Engineer (Score: 7):**  Israa's score reflects demonstrable experience in computer vision, but further information is needed to confirm expertise in all three specified areas.  A score of 7 suggests competency, but more concrete project details would elevate the ranking.\n",
            "\n",
            "\n",
            "4. **Adham Assy - Resume (Score: 7):** Adham's score is a placeholder pending resume review, but the description indicates a good foundational understanding and practical experience based on a hypothetical resume.  Specific project details would be critical to confirming this ranking.\n",
            "\n",
            "\n",
            "5. **Ali Ayman (Score: 7):** Similar to Adham, Ali's score is based on a hypothetical resume indicating promising skills, but lacks the detailed evidence needed to place higher.  Further project details and quantifiable achievements would be required to confirm this ranking.\n",
            "\n",
            "\n",
            "**Important Note:** The scores for several candidates are insufficient due to a lack of resume content.  These candidates cannot be ranked reliably until further information is provided. The rankings above are based on the provided justifications and are subject to change upon review of the complete resumes.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example 3: Rank candidates\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RANKING CANDIDATES\")\n",
        "print(\"=\"*50)\n",
        "criteria = \"computer vision ,object detection, and image segmentation\"\n",
        "print(f\"Criteria: {criteria}\")\n",
        "print(\"Processing...\")\n",
        "\n",
        "ranking = rank_candidates(criteria, top_n=5)\n",
        "\n",
        "print(\"\\nResults:\")\n",
        "print(ranking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "id": "mt5fmrBF-UnZ",
        "outputId": "0146770e-ac21-4e32-86a9-69391d4aef54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "RESUME SCREENING SYSTEM\n",
            "==================================================\n",
            "1. Ask a question about all resumes\n",
            "2. Ask about a specific candidate\n",
            "3. Rank candidates by criteria\n",
            "4. Exit\n",
            "\n",
            "Enter your choice (1-4): 3\n",
            "Enter criteria for ranking candidates: computer vision\n",
            "How many top candidates to show? 3\n",
            "Evaluating candidates... This might take a minute.\n",
            "Evaluating: Hajar_ITI_Resume (8)\n",
            "Evaluating: Aya Attia_(cv)\n",
            "Evaluating: Adham Assy - Resume \n",
            "Evaluating: Resume - Ali Ayman\n",
            "Evaluating: Mohamed_Salama_AI_engineer\n",
            "Evaluating: Mohamed Kamal Elsharkawy\n",
            "Evaluating: Abdallah_Mahmoud_CV\n",
            "Evaluating: Salma-Ibrahim-Zaher-Ai\n",
            "Evaluating: Ali_Salama_resume\n",
            "Evaluating: Mennatullah-Tarek-CV\n",
            "Evaluating: IsraaAbdelghany_AI&ML_Engineer\n",
            "Evaluating: Alaa_Hussien_cv\n",
            "Evaluating: Nagwa Mohamed\n",
            "Evaluating: amr_CV_1.2\n",
            "Evaluating: Alaa_s CV\n",
            "Evaluating: Reham_CV_April_2025-1-\n",
            "Evaluating: Yasmin_Kadry_CV\n",
            "\n",
            "Ranking Results:\n",
            "1. **Mohamed Kamal Elsharkawy (Score: 8):**  Elsharkawy's impressive \"DeepNEye\" project, achieving 99% balanced accuracy in retinal disease diagnosis, showcases a strong command of the entire computer vision pipeline (data preprocessing, model selection, training, evaluation, and deployment).  This, combined with his relevant internship experience, makes him a top contender.\n",
            "\n",
            "2. **amr_CV_1.2 (Score: 8):** amr_CV_1.2 demonstrates strong practical application of computer vision through their self-driving robot project, showcasing hand gesture detection and obstacle avoidance. Relevant coursework and proficiency in key libraries solidify their position as a strong candidate.  The lack of quantifiable metrics prevents them from surpassing Elsharkawy.\n",
            "\n",
            "3. **Yasmin_Kadry_CV (Score: 8):** Yasmin displays solid foundational knowledge and practical experience in core computer vision tasks (image segmentation and classification) using relevant tools and frameworks. Her strong mathematical background further complements her skillset. The absence of specific performance metrics on her projects keeps her slightly below the top two.\n",
            "\n",
            "\n",
            "==================================================\n",
            "RESUME SCREENING SYSTEM\n",
            "==================================================\n",
            "1. Ask a question about all resumes\n",
            "2. Ask about a specific candidate\n",
            "3. Rank candidates by criteria\n",
            "4. Exit\n",
            "\n",
            "Enter your choice (1-4): 4\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# 10. Interactive Function\n",
        "def interactive_questions():\n",
        "    \"\"\"Interactive function to ask questions about the resumes\"\"\"\n",
        "    while True:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"RESUME SCREENING SYSTEM\")\n",
        "        print(\"=\"*50)\n",
        "        print(\"1. Ask a question about all resumes\")\n",
        "        print(\"2. Ask about a specific candidate\")\n",
        "        print(\"3. Rank candidates by criteria\")\n",
        "        print(\"4. Exit\")\n",
        "\n",
        "        choice = input(\"\\nEnter your choice (1-4): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            question = input(\"Enter your question about the resumes: \")\n",
        "            result = query_all_resumes(question)\n",
        "            print(\"\\nAnswer:\")\n",
        "            print(result[\"result\"])\n",
        "\n",
        "        elif choice == '2':\n",
        "            print(\"Available candidates:\")\n",
        "            for i, candidate in enumerate(candidate_names, 1):\n",
        "                print(f\"{i}. {candidate}\")\n",
        "\n",
        "            selection = int(input(\"\\nSelect candidate number: \")) - 1\n",
        "            if 0 <= selection < len(candidate_names):\n",
        "                candidate = candidate_names[selection]\n",
        "                question = input(f\"Enter your question about {candidate}: \")\n",
        "                result = query_specific_candidate(candidate, question)\n",
        "                print(\"\\nAnswer:\")\n",
        "                print(result[\"result\"])\n",
        "            else:\n",
        "                print(\"Invalid selection!\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            criteria = input(\"Enter criteria for ranking candidates: \")\n",
        "            top_n = int(input(\"How many top candidates to show? \"))\n",
        "            print(\"Evaluating candidates... This might take a minute.\")\n",
        "            ranking = rank_candidates(criteria, top_n)\n",
        "            print(\"\\nRanking Results:\")\n",
        "            print(ranking)\n",
        "\n",
        "        elif choice == '4':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice! Please try again.\")\n",
        "\n",
        "# Uncomment to run the interactive function\n",
        "interactive_questions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mDVpLth-Uma"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbnNBhPA7XdX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
